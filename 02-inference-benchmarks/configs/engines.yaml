model:
  name: nvidia/Llama-4-Scout-17B-16E-Instruct-FP8
  type: moe
  active_params: 17B
  total_params: 109B
  precision: fp8

engines:
  vllm:
    port: 8010
    extra_args:
      - "--max-model-len"
      - "4096"
      - "--kv-cache-dtype"
      - "fp8"
      - "--max-num-batched-tokens"
      - "8192"

  sglang:
    port: 8020
    extra_args:
      - "--model-path"
      - "nvidia/Llama-4-Scout-17B-16E-Instruct-FP8"
      - "--port"
      - "8020"
      - "--kv-cache-dtype"
      - "fp8"

  tensorrt_llm:
    port: 8030
    docker_image: "nvcr.io/nvidia/tritonserver:25.02-trtllm-python-py3"
    extra_args: []

benchmark:
  concurrency_levels: [1, 10, 50, 100]
  warmup_requests: 5
  requests_per_prompt: 3
  max_tokens: 256
  temperature: 0.0

hardware:
  gpu: "NVIDIA H200 141GB HBM3e"
  provider: "RunPod"
